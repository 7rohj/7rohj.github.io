---
emoji: 🚫
title: 선형회귀
date: '2022-04-05 02:00:00'
author: 강화정
tags: 공부 자격증 빅데이터 모델링
categories: 빅분기
---

단순 선형 회귀는 독립변수도 하나, 종속변수도 하나인 선형 회귀이다.
실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 잔차라고 부르며
`최적의 회귀 모델을 만든다는 것`은 바로 전체 데이터의 `잔차 합이 최소가 되는 모델을 만든다는 의미`이다.
동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미도 된다.

보통 오류 합을 계산할 때는 `절댓값을 취해서 더하거나`, `오류 값의 제곱을 구해서 더하는 방식`을 취한다.
앞 Meaen Absolute Error, 뒤 Residual Sum of Square

즉, Error^2 = RSS 이다.

RSS는 w0 그리고 w1인 식으로 표현할 수 있으며 이 RSS를 최소로 하는 w0,w1, 즉 회귀 계수를 학습을 통해서
찾는 것이 머신러닝 기반 회귀의 핵심 사항이다.

RSS는 회귀식의 독립변수, 종속변수가 중심 변수가 아니라 회귀 계수인 w 변수가 중심 변수임을 인지하는 것이 매우 중요하다.
학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서는 모두 상수로 간주한다.

회귀에서 이 `RSS`는 `비용(Cost)`이며 w변수로 구성되는 RSS를 비용 함수라고 한다.
머신러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값을 지속해서 감소시키고 최종적으로는
`더 이상 감소하지 않는 최소의 오류 값을 구하는 것`이다. 비용함수를 손실함수(loss function)라고도 한다.

### ※ 비용 최소화하기 - 경사 하강법 (Gradient Descent) ※
경사 하강법은 반복적으로 비용 함수의 반환 값 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다.
최초 오류 값이 100이었다면 두 번째 오류 값은 100보다 작은 90, 세 번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트해 나간다.
그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환한다.

`경사하강법은 수행 시간이 매우 오래 걸린다`는 단점이 있기 때문에 실전에서는 대부분 확률적 경사 하강법을 이용한다.
확률적 경사 하강법은 전체 입력 데이터로 w가 업데이트되는 값을 계산하는 것이 아니라 **일부 데이터만 이용해** w가 업데이트되는 값을 계산하므로
경사 하강법에 비해서 빠른 속도를 보장한다. 따라서 대용량의 데이터의 경우 대부분 `확률적 경사 하강법`이나 `미니 배치 확률적 경사 하강법`을 이용해 최적 비용함수를 도출한다.

<br/>
<br/>
<br/>






